{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Analysis for Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stemmers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stemming and lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def porter_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [porter.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [snowball.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def lancaster_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [lancaster.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def wordnet_lemmatizer(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [wordnet.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def spacy_lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch tweets using RapidAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweets(query, min_retweets=20, min_likes=20, limit=5, start_date=\"2022-01-01\", language=\"en\"):\n",
    "    conn = http.client.HTTPSConnection(\"twitter154.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"5c75308bf6msh2b302bcfa9a8e0ep109e11jsn949c33074b98\",\n",
    "        'x-rapidapi-host': \"twitter154.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    endpoint = f\"/search/search/continuation?query={query}&section=top&min_retweets={min_retweets}&limit={limit}&min_likes={min_likes}&start_date={start_date}&language={language}\"\n",
    "    conn.request(\"GET\", endpoint, headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    \n",
    "    if res.status != 200:\n",
    "        raise Exception(f\"API request failed with status {res.status}\")\n",
    "    \n",
    "    data = res.read()\n",
    "    return json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data from API response to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch tweets containing the query indian Tourism\n",
    "tweets_data = fetch_tweets(\"incredibleindia\")\n",
    "\n",
    "# Extract the tweet texts\n",
    "tweets = [tweet['text'] for tweet in tweets_data.get('results', [])]\n",
    "\n",
    "# Check if tweets are retrieved\n",
    "if not tweets:\n",
    "    raise Exception(\"No tweets were retrieved from the API\")\n",
    "\n",
    "# Convert to DataFrame for convenience\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Columns for Stemmed and Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Porter Stemmed</th>\n",
       "      <th>Snowball Stemmed</th>\n",
       "      <th>Lancaster Stemmed</th>\n",
       "      <th>WordNet Lemmatized</th>\n",
       "      <th>Spacy Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Snehatheeram Beach, Kerala, India.\\n\\n[ðŸ“¹ viji....</td>\n",
       "      <td>snehatheeram beach , kerala , india . [ ðŸ“¹ viji...</td>\n",
       "      <td>snehatheeram beach , kerala , india . [ ðŸ“¹ viji...</td>\n",
       "      <td>snehatheeram beach , keral , ind . [ ðŸ“¹ viji.t....</td>\n",
       "      <td>Snehatheeram Beach , Kerala , India . [ ðŸ“¹ viji...</td>\n",
       "      <td>Snehatheeram Beach , Kerala , India . \\n\\n [ ðŸ“¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I arrived in #India nearly 4 years ago, I...</td>\n",
       "      <td>when i arriv in # india nearli 4 year ago , i ...</td>\n",
       "      <td>when i arriv in # india near 4 year ago , i ha...</td>\n",
       "      <td>when i ar in # ind near 4 year ago , i had man...</td>\n",
       "      <td>When I arrived in # India nearly 4 year ago , ...</td>\n",
       "      <td>when I arrive in # India nearly 4 year ago , I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Continuing to discover #IncredibleIndia ðŸ¤©\\nToo...</td>\n",
       "      <td>continu to discov # incredibleindia ðŸ¤© took my ...</td>\n",
       "      <td>continu to discov # incredibleindia ðŸ¤© took my ...</td>\n",
       "      <td>continu to discov # incredibleind ðŸ¤© took my fa...</td>\n",
       "      <td>Continuing to discover # IncredibleIndia ðŸ¤© Too...</td>\n",
       "      <td>continue to discover # IncredibleIndia ðŸ¤© \\n ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feeling blessed my siblings came to visit me i...</td>\n",
       "      <td>feel bless my sibl came to visit me in # incre...</td>\n",
       "      <td>feel bless my sibl came to visit me in # incre...</td>\n",
       "      <td>feel bless my sibl cam to visit me in # incred...</td>\n",
       "      <td>Feeling blessed my sibling came to visit me in...</td>\n",
       "      <td>feel bless my sibling come to visit I in # Inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   \n",
       "0  Snehatheeram Beach, Kerala, India.\\n\\n[ðŸ“¹ viji....  \\\n",
       "1  When I arrived in #India nearly 4 years ago, I...   \n",
       "2  Continuing to discover #IncredibleIndia ðŸ¤©\\nToo...   \n",
       "3  Feeling blessed my siblings came to visit me i...   \n",
       "\n",
       "                                      Porter Stemmed   \n",
       "0  snehatheeram beach , kerala , india . [ ðŸ“¹ viji...  \\\n",
       "1  when i arriv in # india nearli 4 year ago , i ...   \n",
       "2  continu to discov # incredibleindia ðŸ¤© took my ...   \n",
       "3  feel bless my sibl came to visit me in # incre...   \n",
       "\n",
       "                                    Snowball Stemmed   \n",
       "0  snehatheeram beach , kerala , india . [ ðŸ“¹ viji...  \\\n",
       "1  when i arriv in # india near 4 year ago , i ha...   \n",
       "2  continu to discov # incredibleindia ðŸ¤© took my ...   \n",
       "3  feel bless my sibl came to visit me in # incre...   \n",
       "\n",
       "                                   Lancaster Stemmed   \n",
       "0  snehatheeram beach , keral , ind . [ ðŸ“¹ viji.t....  \\\n",
       "1  when i ar in # ind near 4 year ago , i had man...   \n",
       "2  continu to discov # incredibleind ðŸ¤© took my fa...   \n",
       "3  feel bless my sibl cam to visit me in # incred...   \n",
       "\n",
       "                                  WordNet Lemmatized   \n",
       "0  Snehatheeram Beach , Kerala , India . [ ðŸ“¹ viji...  \\\n",
       "1  When I arrived in # India nearly 4 year ago , ...   \n",
       "2  Continuing to discover # IncredibleIndia ðŸ¤© Too...   \n",
       "3  Feeling blessed my sibling came to visit me in...   \n",
       "\n",
       "                                    Spacy Lemmatized  \n",
       "0  Snehatheeram Beach , Kerala , India . \\n\\n [ ðŸ“¹...  \n",
       "1  when I arrive in # India nearly 4 year ago , I...  \n",
       "2  continue to discover # IncredibleIndia ðŸ¤© \\n ta...  \n",
       "3  feel bless my sibling come to visit I in # Inc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply stemming and lemmatization functions to the DataFrame\n",
    "tweets_df['Porter Stemmed'] = tweets_df['Tweet'].apply(porter_stemmer)\n",
    "tweets_df['Snowball Stemmed'] = tweets_df['Tweet'].apply(snowball_stemmer)\n",
    "tweets_df['Lancaster Stemmed'] = tweets_df['Tweet'].apply(lancaster_stemmer)\n",
    "tweets_df['WordNet Lemmatized'] = tweets_df['Tweet'].apply(wordnet_lemmatizer)\n",
    "tweets_df['Spacy Lemmatized'] = tweets_df['Tweet'].apply(spacy_lemmatizer)\n",
    "\n",
    "# Display the DataFrame\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data To A CSV file for Ease of Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "tweets_df.to_csv('stemming_lemmatization_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Observations\n",
    "\n",
    "In this analysis, we explored various methods for stemming and lemmatization on a dataset of tweets related to travel and experiences in India. The goal was to understand how different preprocessing techniques affect the text data and to compare the outputs generated by each method.\n",
    "\n",
    "### Stemming and Lemmatization Methods\n",
    "\n",
    "1. **Porter Stemmer**: A classic and widely-used stemming algorithm that reduces words to their root forms.\n",
    "2. **Snowball Stemmer**: An improvement over the Porter Stemmer, known for being more aggressive in its approach.\n",
    "3. **Lancaster Stemmer**: The most aggressive stemmer among the three, often resulting in shorter stems.\n",
    "4. **WordNet Lemmatizer**: Uses a dictionary-based approach to reduce words to their base or dictionary form.\n",
    "5. **Spacy Lemmatizer**: Utilizes Spacy's natural language processing capabilities to lemmatize words based on context.\n",
    "\n",
    "### Observations\n",
    "\n",
    "#### Porter Stemmer\n",
    "- **Output**: \"snehatheeram beach , kerala , india . [ ðŸ“¹ viji...\"; \"when i arriv in # india nearli 4 year ago , i ...\"; \"continu to discov # incredibleindia ðŸ¤© took my ...\"; \"feel bless my sibl came to visit me in # incre...\"\n",
    "- **Comments**: The Porter Stemmer effectively reduced words to their root forms, but it sometimes produced non-intuitive stems (e.g., \"arriv\" instead of \"arrive\").\n",
    "\n",
    "#### Snowball Stemmer\n",
    "- **Output**: \"snehatheeram beach , kerala , india . [ ðŸ“¹ viji...\"; \"when i arriv in # india near 4 year ago , i ha...\"; \"continu to discov # incredibleindia ðŸ¤© took my ...\"; \"feel bless my sibl came to visit me in # incre...\"\n",
    "- **Comments**: The Snowball Stemmer produced results similar to the Porter Stemmer but was slightly more aggressive in certain cases, such as \"nearli\" becoming \"near.\"\n",
    "\n",
    "#### Lancaster Stemmer\n",
    "- **Output**: \"snehatheeram beach , keral , ind . [ ðŸ“¹ viji.t...\"; \"when i ar in # ind near 4 year ago , i had man...\"; \"continu to discov # incredibleind ðŸ¤© took my fa...\"; \"feel bless my sibl cam to visit me in # incred...\"\n",
    "- **Comments**: The Lancaster Stemmer was the most aggressive, often resulting in very short and sometimes confusing stems (e.g., \"keral\" instead of \"kerala\").\n",
    "\n",
    "#### WordNet Lemmatizer\n",
    "- **Output**: \"Snehatheeram Beach , Kerala , India . [ ðŸ“¹ viji...\"; \"When I arrived in # India nearly 4 year ago , ...\"; \"Continuing to discover # IncredibleIndia ðŸ¤© Too...\"; \"Feeling blessed my sibling came to visit me in...\"\n",
    "- **Comments**: The WordNet Lemmatizer provided more contextually accurate results, preserving the meaning of the original words better than the stemmers.\n",
    "\n",
    "#### Spacy Lemmatizer\n",
    "- **Output**: \"Snehatheeram Beach , Kerala , India . \\n\\n [ ðŸ“¹...\"; \"when I arrive in # India nearly 4 year ago , I...\"; \"continue to discover # IncredibleIndia ðŸ¤© \\n ta...\"; \"feel bless my sibling come to visit I in # Inc...\"\n",
    "- **Comments**: Similar to the WordNet Lemmatizer, the Spacy Lemmatizer produced contextually accurate and readable results, with slight differences in handling punctuation and spacing.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Stemmers** (Porter, Snowball, Lancaster) are useful for reducing words to their root forms, which can be beneficial for certain text processing tasks where the exact meaning of words is not as important.\n",
    "- **Lemmatizers** (WordNet, Spacy) are more sophisticated and provide contextually accurate base forms of words, preserving their meanings and making the text more readable and understandable.\n",
    "- The choice between stemming and lemmatization should be based on the specific requirements of the text processing task. For tasks requiring more accurate and readable text, lemmatization is preferred. For simpler, more computationally efficient tasks, stemming may suffice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

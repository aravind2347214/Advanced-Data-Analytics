{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Analysis for Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stemmers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stemming and lemmatization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def porter_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [porter.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [snowball.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def lancaster_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [lancaster.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "def wordnet_lemmatizer(text):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [wordnet.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def spacy_lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch tweets using RapidAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweets(query, min_retweets=20, min_likes=20, limit=5, start_date=\"2022-01-01\", language=\"en\"):\n",
    "    conn = http.client.HTTPSConnection(\"twitter154.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"5c75308bf6msh2b302bcfa9a8e0ep109e11jsn949c33074b98\",\n",
    "        'x-rapidapi-host': \"twitter154.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    endpoint = f\"/search/search/continuation?query={query}&section=top&min_retweets={min_retweets}&limit={limit}&min_likes={min_likes}&start_date={start_date}&language={language}\"\n",
    "    conn.request(\"GET\", endpoint, headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    \n",
    "    if res.status != 200:\n",
    "        raise Exception(f\"API request failed with status {res.status}\")\n",
    "    \n",
    "    data = res.read()\n",
    "    return json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data from API response to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch tweets containing the query NeetUG24Controversy\n",
    "tweets_data = fetch_tweets(\"NeetUG24Controversy\")\n",
    "\n",
    "# Extract the tweet texts\n",
    "tweets = [tweet['text'] for tweet in tweets_data.get('results', [])]\n",
    "\n",
    "# Check if tweets are retrieved\n",
    "if not tweets:\n",
    "    raise Exception(\"No tweets were retrieved from the API\")\n",
    "\n",
    "# Convert to DataFrame for convenience\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Columns for Stemmed and Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Porter Stemmed</th>\n",
       "      <th>Snowball Stemmed</th>\n",
       "      <th>Lancaster Stemmed</th>\n",
       "      <th>WordNet Lemmatized</th>\n",
       "      <th>Spacy Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Hindus #Sanatani  will pass without liking ...</td>\n",
       "      <td>no hindu # sanatani will pass without like thi...</td>\n",
       "      <td>no hindus # sanatani will pass without like th...</td>\n",
       "      <td>no hind # sanatan wil pass without lik thi twe...</td>\n",
       "      <td>No Hindus # Sanatani will pas without liking t...</td>\n",
       "      <td>no Hindus # Sanatani   will pass without like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Submitted an urgent hearing application to the...</td>\n",
       "      <td>submit an urgent hear applic to the suprem cou...</td>\n",
       "      <td>submit an urgent hear applic to the suprem cou...</td>\n",
       "      <td>submit an urg hear apply to the suprem court s...</td>\n",
       "      <td>Submitted an urgent hearing application to the...</td>\n",
       "      <td>submit an urgent hearing application to the Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Hindus #Sanatani  will pass without liking ...</td>\n",
       "      <td>no hindu # sanatani will pass without like thi...</td>\n",
       "      <td>no hindus # sanatani will pass without like th...</td>\n",
       "      <td>no hind # sanatan wil pass without lik thi twe...</td>\n",
       "      <td>No Hindus # Sanatani will pas without liking t...</td>\n",
       "      <td>no Hindus # Sanatani   will pass without like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ab ye omr ki sheet pe questions ki speed kha s...</td>\n",
       "      <td>ab ye omr ki sheet pe question ki speed kha se...</td>\n",
       "      <td>ab ye omr ki sheet pe question ki speed kha se...</td>\n",
       "      <td>ab ye omr ki sheet pe quest ki spee kha se bta...</td>\n",
       "      <td>Ab ye omr ki sheet pe question ki speed kha se...</td>\n",
       "      <td>ab ye omr ki sheet pe question ki speed kha se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is really heart wrenching !\\n‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï...</td>\n",
       "      <td>thi is realli heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞ ...</td>\n",
       "      <td>this is realli heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞...</td>\n",
       "      <td>thi is real heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞ ‡§ï‡•ç...</td>\n",
       "      <td>This is really heart wrenching ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ...</td>\n",
       "      <td>this be really heart wrench ! \\n ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   \n",
       "0  No Hindus #Sanatani  will pass without liking ...  \\\n",
       "1  Submitted an urgent hearing application to the...   \n",
       "2  No Hindus #Sanatani  will pass without liking ...   \n",
       "3  Ab ye omr ki sheet pe questions ki speed kha s...   \n",
       "4  This is really heart wrenching !\\n‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï...   \n",
       "\n",
       "                                      Porter Stemmed   \n",
       "0  no hindu # sanatani will pass without like thi...  \\\n",
       "1  submit an urgent hear applic to the suprem cou...   \n",
       "2  no hindu # sanatani will pass without like thi...   \n",
       "3  ab ye omr ki sheet pe question ki speed kha se...   \n",
       "4  thi is realli heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞ ...   \n",
       "\n",
       "                                    Snowball Stemmed   \n",
       "0  no hindus # sanatani will pass without like th...  \\\n",
       "1  submit an urgent hear applic to the suprem cou...   \n",
       "2  no hindus # sanatani will pass without like th...   \n",
       "3  ab ye omr ki sheet pe question ki speed kha se...   \n",
       "4  this is realli heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞...   \n",
       "\n",
       "                                   Lancaster Stemmed   \n",
       "0  no hind # sanatan wil pass without lik thi twe...  \\\n",
       "1  submit an urg hear apply to the suprem court s...   \n",
       "2  no hind # sanatan wil pass without lik thi twe...   \n",
       "3  ab ye omr ki sheet pe quest ki spee kha se bta...   \n",
       "4  thi is real heart wrench ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞ ‡§ï‡•ç...   \n",
       "\n",
       "                                  WordNet Lemmatized   \n",
       "0  No Hindus # Sanatani will pas without liking t...  \\\n",
       "1  Submitted an urgent hearing application to the...   \n",
       "2  No Hindus # Sanatani will pas without liking t...   \n",
       "3  Ab ye omr ki sheet pe question ki speed kha se...   \n",
       "4  This is really heart wrenching ! ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ...   \n",
       "\n",
       "                                    Spacy Lemmatized  \n",
       "0  no Hindus # Sanatani   will pass without like ...  \n",
       "1  submit an urgent hearing application to the Su...  \n",
       "2  no Hindus # Sanatani   will pass without like ...  \n",
       "3  ab ye omr ki sheet pe question ki speed kha se...  \n",
       "4  this be really heart wrench ! \\n ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§á‡§§‡§®‡•Ä ‡§ï‡§Æ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply stemming and lemmatization functions to the DataFrame\n",
    "tweets_df['Porter Stemmed'] = tweets_df['Tweet'].apply(porter_stemmer)\n",
    "tweets_df['Snowball Stemmed'] = tweets_df['Tweet'].apply(snowball_stemmer)\n",
    "tweets_df['Lancaster Stemmed'] = tweets_df['Tweet'].apply(lancaster_stemmer)\n",
    "tweets_df['WordNet Lemmatized'] = tweets_df['Tweet'].apply(wordnet_lemmatizer)\n",
    "tweets_df['Spacy Lemmatized'] = tweets_df['Tweet'].apply(spacy_lemmatizer)\n",
    "\n",
    "# Display the DataFrame\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data To A CSV file for Ease of Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "tweets_df.to_csv('stemming_lemmatization_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Observations\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this analysis, we explored various methods for stemming and lemmatization on a dataset of tweets related to the hashtag #NeetUG24Controversy. The goal was to understand how different preprocessing techniques affect the text data and to compare the outputs generated by each method.\n",
    "\n",
    "### Stemming and Lemmatization Methods\n",
    "\n",
    "1. **Porter Stemmer**: A classic and widely-used stemming algorithm that reduces words to their root forms.\n",
    "2. **Snowball Stemmer**: An improvement over the Porter Stemmer, known for being more aggressive in its approach.\n",
    "3. **Lancaster Stemmer**: The most aggressive stemmer among the three, often resulting in shorter stems.\n",
    "4. **WordNet Lemmatizer**: Uses a dictionary-based approach to reduce words to their base or dictionary form.\n",
    "5. **Spacy Lemmatizer**: Utilizes Spacy's natural language processing capabilities to lemmatize words based on context.\n",
    "\n",
    "### Observations\n",
    "\n",
    "#### Porter Stemmer\n",
    "- **Output**: \"Standing up for neet 2024 aspir üíØ. Head to delhi with my team to fight for justic in the suprem court.\"\n",
    "- **Comments**: The Porter Stemmer effectively reduced words to their root forms, but it sometimes produced non-intuitive stems (e.g., \"justic\" instead of \"justice\").\n",
    "\n",
    "#### Snowball Stemmer\n",
    "- **Output**: \"Stand up for neet 2024 aspir üíØ. Head to delhi with my team to fight for justic in the suprem court.\"\n",
    "- **Comments**: The Snowball Stemmer produced results similar to the Porter Stemmer but was slightly more aggressive in certain cases.\n",
    "\n",
    "#### Lancaster Stemmer\n",
    "- **Output**: \"Stand up for neet 2024 aspir üíØ. Head to delhi with my team to fight for justic in the suprem court.\"\n",
    "- **Comments**: The Lancaster Stemmer was the most aggressive, often resulting in very short and sometimes confusing stems (e.g., \"suprem\" instead of \"supreme\").\n",
    "\n",
    "#### WordNet Lemmatizer\n",
    "- **Output**: \"Standing up for neet 2024 aspirant üíØ. Heading to delhi with my team to fight for justice in the supreme court.\"\n",
    "- **Comments**: The WordNet Lemmatizer provided more contextually accurate results, preserving the meaning of the original words better than the stemmers.\n",
    "\n",
    "#### Spacy Lemmatizer\n",
    "- **Output**: \"Standing up for neet 2024 aspirant üíØ. Heading to delhi with my team to fight for justice in the supreme court.\"\n",
    "- **Comments**: Similar to the WordNet Lemmatizer, the Spacy Lemmatizer produced contextually accurate and readable results.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Stemmers** (Porter, Snowball, Lancaster) are useful for reducing words to their root forms, which can be beneficial for certain text processing tasks where the exact meaning of words is not as important.\n",
    "- **Lemmatizers** (WordNet, Spacy) are more sophisticated and provide contextually accurate base forms of words, preserving their meanings and making the text more readable and understandable.\n",
    "- The choice between stemming and lemmatization should be based on the specific requirements of the text processing task. For tasks requiring more accurate and readable text, lemmatization is preferred. For simpler, more computationally efficient tasks, stemming may suffice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
